{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Predicting Continuous Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $N$ observations, $\\xv_n$, for $n=1,\\ldots,N$, and target values,\n",
    "$t_n$, for $n=1,\\ldots,N$, what is the simplest model,\n",
    "$g(\\xv)$, you can think of?\n",
    "\n",
    "$$\n",
    "g(\\xv) = 0\n",
    "$$\n",
    "\n",
    "or maybe\n",
    "\n",
    "$$\n",
    "g(\\xv) = c\n",
    "$$\n",
    "\n",
    "What is next simplest model?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   g(\\xv;\\wv) &= w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_D x_D \\\\\n",
    "   &= w_0 + \\sum_{i=1}^D w_i x_i \\\\\n",
    "   & = \\sum_{i=0}^D w_i x_i \\mbox{, where } x_0 = 1 \\\\\n",
    "   &= \\wv^T \\xv\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "  * This is nice because it is linear in the parameters $\\wv$; optimizations based on derivatives might be solvable analytically.\n",
    "  * This is not so nice, because it is also linear in the inputs, $\\xv$; greatly limits the complexity of the model.\n",
    "  *  But, a model linear in the inputs might be the best you can do for many cases, such as a sparsely sampled distribution, process, population, thing...whatever it is you want to model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Data Samples with a Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs480/notebooks/springs.png\">\n",
    "\n",
    "Force exerted by spring is proportional to its length. The potential\n",
    "energy stored in the spring is proportinal to the square of its length.\n",
    "Say we want the rod to settle at position that minimizes the potential\n",
    "energy in the springs.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{n=1}^N (t_n - g(\\xv_n;\\wv))^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If $g$ is an affine (linear + constant) function of $x$,\n",
    "$$\n",
    "    g(\\xv;\\wv) = w_0 + w_1 x\n",
    "$$\n",
    "with parameters $\\wv = (w_0, w_1)$, which parameter values give best fit?\n",
    "$$\n",
    "    \\wv_{\\mbox{best}} = \\argmin{\\wv} \\sum_{n=1}^N (t_n - g(x_n ; \\wv))^2\n",
    "$$\n",
    "\n",
    "Set derivative (gradient) with respect to $\\wv$ to zero and\n",
    "solve for $\\wv$.  Let's do this with matrices.  \n",
    "\n",
    "Collect all targets into matrix $T$ and $x$ samples into matrix\n",
    "$X$. ($N$=number samples, $D$=sample dimensionality)\n",
    "$$\n",
    "  \\begin{align*}\n",
    "    T &= \\begin{bmatrix}\n",
    "      t_1 \\\\ t_2 \\\\ \\vdots \\\\ t_N\n",
    "    \\end{bmatrix} \\\\\n",
    "    X &= \\begin{bmatrix}\n",
    "      x_{1,0} & x_{1,1} & x_{1,2} & \\dotsc & x_{1,D} \\\\\n",
    "      x_{2,0} & x_{2,1} & x_{2,2} & \\dotsc & x_{2,D} \\\\\n",
    "      \\vdots \\\\\n",
    "      x_{N,0} & x_{N,1} & x_{N,2} & \\dotsc & x_{N,D}\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\wv &= \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{bmatrix}\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Collection of all differences is $T - X\\wv$, which is an $N \\times\n",
    "1$ matrix.  To form the square of all values and add them up, just\n",
    "do a dot product $(T-X\\wv)^T (T-X\\wv)$.  This only works if the value we are predicting is a scalar, which means $T$ is a column matrix.  If we want to predict more than one value for each sample, $T$ will have more than one column.  Let's continue with the derivation assuming $T$ has $K$ columns, meaning we want a linear model with $K$ outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best value for $\\wv$, we take the derivative the the sum of squared error objective, set it equal to 0 and solve for $\\wv$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sum_{n=1}^N (\\tv_n - g(\\xv_n;\\wv))^2}{\\partial \\wv} &= -2 \\sum_{n=1}^N (\\tv_n - g(\\xv_n;\\wv) \\frac{\\partial g(\\xv_n;\\wv)}{\\partial \\wv}\\\\\n",
    "&= -2 \\sum_{n=1}^N (\\tv_n - \\xv_n^T \\wv) \\frac{\\partial \\xv_n^T \\wv}{\\partial \\wv}\\\\\n",
    "&= -2 \\sum_{n=1}^N (\\tv_n - \\xv_n^T \\wv) \\xv_n^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here's where we get the benefit of expressing the $\\xv_n$ and $t_n$ samples as matrices. The sum can be performed with a dot product:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sum_{n=1}^N (\\tv_n - g(\\xv_n;\\wv))^2}{\\partial \\wv} \n",
    "&= -2 \\sum_{n=1}^N (\\tv_n - \\xv_n^T \\wv) \\xv_n^T\\\\\n",
    "&= -2 \\Xv^T (\\Tv - \\Xv \\wv)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Check the sizes and shapes of each matrix in the last equation above.\n",
    "\n",
    "Now we can set this equal to zero and solve for $\\wv$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "-2 \\Xv^T (\\Tv - \\Xv \\wv) &= 0\\\\\n",
    "\\Xv^T (\\Tv - \\Xv \\wv) &= 0\\\\\n",
    "\\Xv^T \\Tv &= \\Xv^T \\Xv \\wv\\\\\n",
    "\\wv &= (\\Xv^T \\Xv)^{-1} \\Xv^T \\Tv\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, in python\n",
    "\n",
    "    w = np.dot( np.linalg.inv(np.dot(X.T, X)), np.dot(X.T,T) )\n",
    "\n",
    "or, you may use the *solve* function that assumes $\\Xv^T \\Xv$ is full rank (no linearly dependent columns),\n",
    "\n",
    "    w = np.linalg.solve(np.dot(X.T,X), np.dot(X.T, T))\n",
    "\n",
    "or, better yet, use the *lstsq* function that does not make that assumption. \n",
    "\n",
    "    w = np.linalg.lstsq(np.dot(X.T,X), np.dot(X.T, T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the [automobile data set](http://mlr.cs.umass.edu/ml/datasets/Auto+MPG) at UCI Machine Learning Database \n",
    "Repository. Download\n",
    "*auto-mpg.data* and *auto-mpg.names* from the \"Data Folder\" link\n",
    "near the top of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget http://mlr.cs.umass.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n",
    "!wget http://mlr.cs.umass.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at *auto-mpg.names*.  There you will learn that there are 398 samples, each with 8 numerical attributes and one string attribute. Their names are\n",
    "\n",
    "   1. mpg:           continuous\n",
    "   2. cylinders:     multi-valued discrete\n",
    "   3. displacement:  continuous\n",
    "   4. horsepower:    continuous\n",
    "   5. weight:        continuous\n",
    "   6. acceleration:  continuous\n",
    "   7. model year:    multi-valued discrete\n",
    "   8. origin:        multi-valued discrete\n",
    "   9. car name:      string (unique for each instance)\n",
    "\n",
    "First step, read it into python and look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a few lines of the data file to figure out how to read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!head -40 auto-mpg.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying on the *pandas* package to read this data, let's try using the simpler *numpy.loadtxt* function.\n",
    "We see that each line contains a sample consisting of 8 numbers and\n",
    "one string.  However, first we have to figure out how to deal with the\n",
    "missing values. The documentation for *numpy.loadtxt* reveals that\n",
    "the *converters* argument can be used to specify a function to apply\n",
    "to the values from particular columns of data.  We can use this to\n",
    "deal with the missing values.  Looking at *auto-mpg.data* we see\n",
    "that the missing values are marked with question marks.  These are all in the fourth column, the horsepower attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple function that translates '?' to NaN (*np.nan*) and anything else is converted to a floating point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.32\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "def missingIsNan(s):\n",
    "    if s == b'?':  # single character read as b'?' != '?'\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(s)\n",
    "    \n",
    "print(missingIsNan('12.32'))\n",
    "print(missingIsNan(b'?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, why not get fancy and write a one-liner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.32\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "def missingIsNan(s):\n",
    "    return np.nan if s == b'?' else float(s)\n",
    "\n",
    "print(missingIsNan('12.32'))\n",
    "print(missingIsNan(b'?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *converters* argument to *np.loadtxt* accepts a python dictionary, which is python's associative array structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a', 2: 'yo', 3: 42, 4: <function __main__.missingIsNan>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {1: 'a', 2: 'yo', 3: 42, 4: missingIsNan}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.missingIsNan>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[4](b'?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also restrict *np.loadtxt* to reading just the first 8 columns to avoid dealing with the string in the 9th column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auto-mpg.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e7be0ec9f1a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auto-mpg.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmissingIsNan\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auto-mpg.data'"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('auto-mpg.data', usecols=range(8), converters={3: missingIsNan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the precision that ipython uses to display floating point values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done before, we must find the missing values.  Let's just remove the samples with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.isnan(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.isnan(data), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this result tell us?\n",
    "\n",
    "Yep, all 6 *NaN* values are in the fourth column, the horsepower column.\n",
    "\n",
    "Let's just\n",
    "remove those 6 samples from the array.  To do this build a boolean\n",
    "mask of length equal to the number of rows in *data* that is\n",
    "*False* except for any row that contains a NaN.  We can use the\n",
    "*np.any* or *np.all* methods to combine boolean values in each row or column. First, always try\n",
    "such ideas in multiple steps, checking each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nans = np.isnan(data)\n",
    "nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nans.any(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nans.any(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the one we want, a boolean value for each row, or sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goodRowsMask = nans.any(axis=1)\n",
    "goodRowsMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataNew = data[goodRowsMask,:]\n",
    "dataNew.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute!  This gives us only 6 samples.  We wanted all samples but these 6, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's change all *False* values to *True* and *True* values to *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goodRowsMask = np.logical_not(goodRowsMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataNew = data[goodRowsMask,:]\n",
    "dataNew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataNew[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.isnan(dataNew),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the next step after reading data into python is to visualize\n",
    "it.  One thing we can do is just plot the value of each attribute in a\n",
    "separate graph.  Let's make an array of column names to label the y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names =  ['mpg','cylinders','displacement','horsepower','weight',\n",
    "          'acceleration','year','origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "nrow,ncol = dataNew.shape\n",
    "for c in range(ncol):\n",
    "    plt.subplot(3,3, c+1)\n",
    "    plt.plot(dataNew[:,c])\n",
    "    plt.ylabel(names[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting to you in these graphs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict miles per gallon, *mpg*, from the other attributes.  To set this up, let's make a 392 x 1 column vector, *T*, of target values containing all of the *mpg* values, and a 392 x 7 matrix, *X*, to hold the inputs to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = dataNew[:, 0:1]  # dataNew[:,0] results in a one-dimensional matrix, dataNew[:,0:1] preserves its two-dimensional nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataNew[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xnames = names[1:]\n",
    "Tname = names[0]\n",
    "Xnames,Tname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if a linear model makes some sense by plotting the target values versus each of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for c in range(X.shape[1]):\n",
    "    plt.subplot(3,3, c+1)\n",
    "    plt.plot(X[:,c], T, 'o', alpha=0.5)\n",
    "    plt.ylabel(Tname)\n",
    "    plt.xlabel(Xnames[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?  Are there any linear relationships between the individual input variables and the target variable?  Do they make sense, given your knowledge of automobiles?\n",
    "\n",
    "Now, to build the linear model.  First, let's tack on a column of constant 1 values to the left side of *X*.  With this addition, our matrix multiplication takes care of the $w_0$ coefficient as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1 = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "X.shape, X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's add a name to *Xnames*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xnames.insert(0, 'bias')\n",
    "Xnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to fit a linear model to all of the data and check to see\n",
    "how accurately we predict *mpg* for each sample.  However, this will\n",
    "give a much too optimistic expectation of how well the model will do\n",
    "on new data.\n",
    "\n",
    "A much better way to evaluate a model is to remove, or hold out, some\n",
    "of the samples from the data set used to fit the model.  Then apply\n",
    "the model to the held out samples and compare the predicted *mpg*\n",
    "with the actual *mpg*.  Call these held out samples the **test set**\n",
    "and the other samples used to fit the model the **train set**.  \n",
    "\n",
    "How should we partition the data into these two disjoint subsets?  A\n",
    "common practice is to randomly select 80% of the samples as training\n",
    "samples and use the remaining 20% of the samples as testing samples.\n",
    "\n",
    "To partition our samples (rows of *X* and *T*) into training and test sets, let's deal with just the row indices.  Then we will use the same selected row indices to slice out corresponding rows of *X* and *T*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrows = X1.shape[0]\n",
    "nTrain = int(round(nrow*0.8))\n",
    "nTest = nrow - nTrain\n",
    "nTrain,nTest,nTrain+nTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = np.arange(nrows)\n",
    "np.random.shuffle(rows)\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainIndices = rows[:nTrain]\n",
    "testIndices = rows[nTrain:]\n",
    "trainIndices,testIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the training and testing sets are disjoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.intersect1d(trainIndices, testIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = X1[trainIndices,:]\n",
    "Ttrain = T[trainIndices,:]\n",
    "Xtest = X1[testIndices,:]\n",
    "Ttest = T[testIndices,:]\n",
    "Xtrain.shape,Ttrain.shape, Xtest.shape,Ttest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve for $\\wv$ in the equation  $X^T X \\wv =  X^T T$.\n",
    "This is done with the *numpy.linalg.lstsq* function.  Don't be confused by the *T*s.  Remember *something.T* is the\n",
    "transpose of *something*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.linalg.lstsq(np.dot(Xtrain.T,Xtrain), np.dot(Xtrain.T, Ttrain))\n",
    "w = w[0] # to only keep the weights, and discard other information returned by lstsq\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for wi,name in zip(w.flat,Xnames):\n",
    "    print('{:8.3f}  {:s}'.format(wi,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you figure out what *np.linalg.lstsq* is doing?  Try finding the source code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!locate linalg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my version I see documentation for *lstsq* that states\n",
    "\n",
    "    Return the least-squares solution to a linear matrix equation.\n",
    "\n",
    "    Solves the equation `a x = b` by computing a vector `x` that\n",
    "    minimizes the Euclidean 2-norm `|| b - a x ||^2`.  The equation may\n",
    "    be under-, well-, or over- determined (i.e., the number of\n",
    "    linearly independent rows of `a` can be less than, equal to, or\n",
    "    greater than its number of linearly independent columns).  If `a`\n",
    "    is square and of full rank, then `x` (but for round-off error) is\n",
    "    the \"exact\" solution of the equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a linear model, which is simply $w$, let's use it to predict *mpg* for some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = np.dot(X1[:4,:],w)\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these predictions compare with the actual *mpg* values?  We can either make a two column matrix, or use a *for* loop to print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.hstack(( predict, Ttrain[:4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('{:^5} {:^5}'.format('P','T'))\n",
    "for (p,t) in zip(predict,Ttrain[0:4,:]):\n",
    "    # print(p,t)\n",
    "    print('{:5.2f} {:5.2f}'.format(p[0],t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try all of the test data and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = np.dot(Xtest, w)\n",
    "predict.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(predict,Ttest,'o')\n",
    "plt.xlabel('Predicted MPG')\n",
    "plt.ylabel('Actual MPG')\n",
    "# add a 45 degree line\n",
    "a = max(min(predict),min(Ttest))\n",
    "b = min(max(predict),max(Ttest))\n",
    "plt.plot([a,b],[a,b], 'r', linewidth=3,alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby!  But, how about a numerical measure of accuracy?  \n",
    "\n",
    "Right, just calculate the root-mean-square error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt( np.mean( (np.dot(Xtest,w) - Ttest)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we are about 2.8 mpg off in our predictions, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Attributes (Input Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again are the weights we learned before for predicting *mpg*.\n",
    "Can we use these to decide which attibutes are most important?  Can\n",
    "we remove some from the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for wi,name in zip(w.flat,Xnames):\n",
    "    print('{:8.3f}  {:s}'.format(wi,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps year and origin are the most significant.  Does this make sense?\n",
    "\n",
    "A weight\n",
    "magnitude is determined not only by the linear relationship between the corresponding input variable with\n",
    "the target, but also by the variable's range.\n",
    "\n",
    "What are the ranges of the input variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n,mn,mx in zip(Xnames,np.min(X1,axis=0),np.max(X1,axis=0)):\n",
    "    print('{:>20} {:8.2f} {:8.2f}'.format(n,mn,mx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight for *weight* is the smallest magnitude, but the range of its values are the largest.\n",
    "\n",
    "The weight for *origin* is the largest, but the range of its values are the smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could remove the effect of the ranges on the weight magnitudes, we could use the weight magnitudes to judge the relative importance of each input variable. How?\n",
    "\n",
    "A common approach is to *standardize* the input variables by adjusting the values to have zero mean and unit variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this correctly when partitioning data into training and testing sets, we must always calculate means and standard deviations using only the training set, and use the same means and standard deviations when standardizing the testing set.  Remember, you must not use any information about the testing set when building a model.  If you do, your test error will be lower than it will be when you truly see new data.\n",
    "\n",
    "You can do this by storing the means and standard deviations obtained from the training data, and just use them when standardizing the testing data.  Don't forget to add the column of 1's after standardizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = np.mean(X,axis=0)\n",
    "stds = np.std(X,axis=0)\n",
    "Xs = (X - means) / stds\n",
    "Xs1 = np.hstack((np.ones((Xs.shape[0],1)), Xs))                \n",
    "w = np.linalg.lstsq( np.dot(Xs1.T,Xs1), np.dot(Xs1.T, T) )[0]\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to construct functions for standardizing that include the calculated means and standard deviations as local variables, by using [function closures](http://www.openbookproject.net/py4fun/decorator/decorator.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeStandardize(X):\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "    \n",
    "    def standardize(origX):\n",
    "        return (origX - means) / stds\n",
    "    \n",
    "    def unStandardize(stdX):\n",
    "        return stds * stdX + means\n",
    "    \n",
    "    return (standardize, unStandardize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with *X* again, and tack on the column of 1's after dividing data into training and testing partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = X[trainIndices,:]\n",
    "Ttrain = T[trainIndices,:]\n",
    "Xtest = X[testIndices,:]\n",
    "Ttest = T[testIndices,:]\n",
    "\n",
    "(standardize, unStandardize) = makeStandardize(Xtrain)\n",
    "\n",
    "XtrainS = standardize(Xtrain)\n",
    "XtestS = standardize(Xtest)\n",
    "\n",
    "np.mean(XtrainS,axis=0), np.std(XtrainS,axis=0), np.mean(XtestS,axis=0), np.std(XtestS,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the means and standard deviations for the testing set are not as close to 0 and 1 as they are for the training set.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tack on the column of 1's, solve for *w*, and examine the weight magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XtrainS1 = np.hstack((np.ones((XtrainS.shape[0],1)), XtrainS))\n",
    "XtestS1 = np.hstack((np.ones((XtestS.shape[0],1)), XtestS))\n",
    "w = np.linalg.lstsq( np.dot(XtrainS1.T,XtrainS1), np.dot(XtrainS1.T, Ttrain))[0]  # see this [0]?\n",
    "for wi,name in zip(w.flat,Xnames):\n",
    "    print('{:8.3f}  {:s}'.format(wi,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what do you observe about the relative magnitudes?  If you had a ton of input variables, it would be easier to see if we sorted them by their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.abs(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argsort(np.abs(w.flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argsort(np.abs(w.flat))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedOrder = np.argsort(np.abs(w.flat))[::-1]\n",
    "Xnames = np.array(Xnames)\n",
    "for wi,name in zip(w.flat[sortedOrder],Xnames[sortedOrder]):\n",
    "    print('{:8.3f}  {:s}'.format(wi,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Target Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of matrix equations now shines.   Previously $T$ was an $N\n",
    "\\times 1$ matrix of target \n",
    "values, one per sample.  \n",
    "\n",
    "Just add additional columns of target values for each\n",
    "target component.  So $T$ becomes an $N \\times K$ matrix,\n",
    "if there are $K$ output components.  Each row is one\n",
    "$K$-dimensional sample.  \n",
    "\n",
    "Looking again at the solution for $\\wv$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    " \\wv &= (X^T X)^{-1} X^T T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "we see that this changes $\\wv$ from its original form of $D\n",
    "\\times 1$ to $D \\times K$.\n",
    "\n",
    "Let's use this to predict miles per gallon **and** horsepower.\n",
    "\n",
    "First, let's assemble the data for predicting MPG and HP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataNew[:,[1,2,4,5,6,7]]\n",
    "T = dataNew[:,[0,3]] \n",
    "Tnames = [names[0], names[3]]\n",
    "X.shape,Xnames,T.shape,Tnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = X[trainIndices,:]\n",
    "Ttrain = T[trainIndices,:]\n",
    "Xtest = X[testIndices,:]\n",
    "Ttest = T[testIndices,:]\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "standardize,_ = makeStandardize(Xtrain)\n",
    "XtrainS = standardize(Xtrain)\n",
    "XtestS = standardize(Xtest)\n",
    "XtrainS1 = np.hstack((np.ones((XtrainS.shape[0],1)), XtrainS))\n",
    "Xnames = np.array(['bias']+names)[[0,2,3,5,6,7,8]]\n",
    "Xnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XtrainS1.shape,Ttrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.linalg.lstsq( np.dot(XtrainS1.T, XtrainS1), np.dot(XtrainS1.T, Ttrain))[0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xnames = np.array(Xnames)\n",
    "for targeti in range(2):\n",
    "    print('\\nTarget {}\\n'.format(Tnames[targeti]))\n",
    "    thisw = w[:,targeti]\n",
    "    sortedOrder = np.argsort(np.abs(thisw))[::-1]\n",
    "    for wi,name in zip(thisw[sortedOrder],Xnames[sortedOrder]):\n",
    "        print('{:8.3f}  {:s}'.format(wi,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try predicting both mpg and horsepower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XtestS1 = np.hstack((np.ones((XtestS.shape[0],1)), XtestS))\n",
    "prediction = np.dot(XtestS1,w)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    plt.subplot(2,1,p+1)\n",
    "    plt.plot(prediction[:,p],Ttest[:,p],'o')\n",
    "    plt.xlabel(\"Predicted \" + Tnames[p])\n",
    "    plt.ylabel(\"Actual \" + Tnames[p])\n",
    "    a = max(min(prediction[:,p]),min(Ttest[:,p]))\n",
    "    b = min(max(prediction[:,p]),max(Ttest[:,p]))\n",
    "    plt.plot([a,b],[a,b],'r',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did we do in terms of RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmseTrain = np.sqrt(np.mean((np.dot(XtrainS1,w) - Ttrain)**2,axis=0))\n",
    "rmseTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmseTest = np.sqrt(np.mean((np.dot(XtestS1,w) - Ttest)**2,axis=0))\n",
    "rmseTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training RMSE: MPG {:4.2f} HP {:4.2f}'.format(*rmseTrain))   #what is that * doing there??\n",
    "print(' Testing RMSE: MPG {:4.2f} HP {:4.2f}'.format(*rmseTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
