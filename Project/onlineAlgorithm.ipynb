{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-21 15:51:54,192] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\checkpoints\\model...\n",
      "\n",
      "Populating replay memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-21 16:00:32,218] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 747 (537008) @ Episode 1/10000, loss: 0.0013130886945873546\n",
      "Episode Reward: 9.0\n",
      "Step 1193 (538201) @ Episode 2/10000, loss: 0.0039718081243336274\n",
      "Episode Reward: 28.0\n",
      "Step 638 (538839) @ Episode 3/10000, loss: 0.0074564456008374694\n",
      "Episode Reward: 8.0\n",
      "Step 978 (539817) @ Episode 4/10000, loss: 0.0026686000637710094\n",
      "Episode Reward: 21.0\n",
      "Step 182 (539999) @ Episode 5/10000, loss: 0.0017242176691070242\n",
      "Copied model parameters to target network.\n",
      "Step 1070 (540887) @ Episode 5/10000, loss: 0.0031045754440128803\n",
      "Episode Reward: 26.0\n",
      "Step 935 (541822) @ Episode 6/10000, loss: 0.0032195870298892265\n",
      "Episode Reward: 14.0\n",
      "Step 1707 (543529) @ Episode 7/10000, loss: 0.0018776498036459088\n",
      "Episode Reward: 38.0\n",
      "Step 714 (544243) @ Episode 8/10000, loss: 0.0263510756194591522\n",
      "Episode Reward: 13.0\n",
      "Step 843 (545086) @ Episode 9/10000, loss: 0.0015223659574985504\n",
      "Episode Reward: 14.0\n",
      "Step 967 (546053) @ Episode 10/10000, loss: 0.0029497803188860416\n",
      "Episode Reward: 16.0\n",
      "Step 913 (546966) @ Episode 11/10000, loss: 0.0012774833012372255\n",
      "Episode Reward: 20.0\n",
      "Step 963 (547929) @ Episode 12/10000, loss: 0.0018270640866830945\n",
      "Episode Reward: 19.0\n",
      "Step 1718 (549647) @ Episode 13/10000, loss: 0.0026324046775698663\n",
      "Episode Reward: 36.0\n",
      "Step 352 (549999) @ Episode 14/10000, loss: 0.0009402023861184716\n",
      "Copied model parameters to target network.\n",
      "Step 1057 (550704) @ Episode 14/10000, loss: 0.0015231177676469088\n",
      "Episode Reward: 27.0\n",
      "Step 948 (551652) @ Episode 15/10000, loss: 0.0138294016942381863\n",
      "Episode Reward: 18.0\n",
      "Step 894 (552546) @ Episode 16/10000, loss: 0.0059254700317978867\n",
      "Episode Reward: 16.0\n",
      "Step 742 (553288) @ Episode 17/10000, loss: 0.0026505754794925457\n",
      "Episode Reward: 13.0\n",
      "Step 849 (554137) @ Episode 18/10000, loss: 0.0013998412759974599\n",
      "Episode Reward: 15.0\n",
      "Step 944 (555081) @ Episode 19/10000, loss: 0.0041805305518209934\n",
      "Episode Reward: 18.0\n",
      "Step 998 (556079) @ Episode 20/10000, loss: 0.0040399250574409964\n",
      "Episode Reward: 22.0\n",
      "Step 972 (557051) @ Episode 21/10000, loss: 0.0034856279380619526\n",
      "Episode Reward: 17.0\n",
      "Step 808 (557859) @ Episode 22/10000, loss: 0.0015094914706423879\n",
      "Episode Reward: 13.0\n",
      "Step 873 (558732) @ Episode 23/10000, loss: 0.0015534153208136559\n",
      "Episode Reward: 17.0\n",
      "Step 1033 (559765) @ Episode 24/10000, loss: 0.0017194533720612526\n",
      "Episode Reward: 24.0\n",
      "Step 234 (559999) @ Episode 25/10000, loss: 0.0012133907293900847\n",
      "Copied model parameters to target network.\n",
      "Step 1086 (560851) @ Episode 25/10000, loss: 0.0018835632363334298\n",
      "Episode Reward: 27.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 00:54:38,273] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000025.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1110 (561961) @ Episode 26/10000, loss: 0.0020231464877724648\n",
      "Episode Reward: 23.0\n",
      "Step 817 (562778) @ Episode 27/10000, loss: 0.0023309942334890366\n",
      "Episode Reward: 14.0\n",
      "Step 931 (563709) @ Episode 28/10000, loss: 0.0015298910439014435\n",
      "Episode Reward: 16.0\n",
      "Step 1149 (564858) @ Episode 29/10000, loss: 0.0037581166252493865\n",
      "Episode Reward: 24.0\n",
      "Step 1387 (566245) @ Episode 30/10000, loss: 0.0078518465161323556\n",
      "Episode Reward: 30.0\n",
      "Step 1171 (567416) @ Episode 31/10000, loss: 0.0023411428555846214\n",
      "Episode Reward: 25.0\n",
      "Step 882 (568298) @ Episode 32/10000, loss: 0.0008527461905032396\n",
      "Episode Reward: 14.0\n",
      "Step 852 (569150) @ Episode 33/10000, loss: 0.00214619701728224756\n",
      "Episode Reward: 14.0\n",
      "Step 849 (569999) @ Episode 34/10000, loss: 0.00356583157554268843\n",
      "Copied model parameters to target network.\n",
      "Step 919 (570069) @ Episode 34/10000, loss: 0.0089018382132053387\n",
      "Episode Reward: 16.0\n",
      "Step 1137 (571206) @ Episode 35/10000, loss: 0.0177932623773813256\n",
      "Episode Reward: 23.0\n",
      "Step 1085 (572291) @ Episode 36/10000, loss: 0.0038746334612369537\n",
      "Episode Reward: 33.0\n",
      "Step 962 (573253) @ Episode 37/10000, loss: 0.0025519696064293385\n",
      "Episode Reward: 23.0\n",
      "Step 1592 (574845) @ Episode 38/10000, loss: 0.0017583565786480904\n",
      "Episode Reward: 41.0\n",
      "Step 890 (575735) @ Episode 39/10000, loss: 0.0029451497830450535\n",
      "Episode Reward: 18.0\n",
      "Step 1125 (576860) @ Episode 40/10000, loss: 0.0020186263136565685\n",
      "Episode Reward: 27.0\n",
      "Step 1435 (578295) @ Episode 41/10000, loss: 0.0015514858532696962\n",
      "Episode Reward: 38.0\n",
      "Step 1001 (579296) @ Episode 42/10000, loss: 0.006486603524535894\n",
      "Episode Reward: 20.0\n",
      "Step 703 (579999) @ Episode 43/10000, loss: 0.0020659510046243668\n",
      "Copied model parameters to target network.\n",
      "Step 800 (580096) @ Episode 43/10000, loss: 0.0013811534736305475\n",
      "Episode Reward: 12.0\n",
      "Step 707 (580803) @ Episode 44/10000, loss: 0.0045015639625489714\n",
      "Episode Reward: 10.0\n",
      "Step 652 (581455) @ Episode 45/10000, loss: 0.0037417938001453876\n",
      "Episode Reward: 9.0\n",
      "Step 920 (582375) @ Episode 46/10000, loss: 0.0018123770132660866\n",
      "Episode Reward: 18.0\n",
      "Step 841 (583216) @ Episode 47/10000, loss: 0.0019042221829295158\n",
      "Episode Reward: 15.0\n",
      "Step 800 (584016) @ Episode 48/10000, loss: 0.0026443456299602985\n",
      "Episode Reward: 16.0\n",
      "Step 1244 (585260) @ Episode 49/10000, loss: 0.0038943602703511715\n",
      "Episode Reward: 25.0\n",
      "Step 972 (586232) @ Episode 50/10000, loss: 0.0019023804925382137\n",
      "Episode Reward: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 03:29:48,233] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000050.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 584 (586816) @ Episode 51/10000, loss: 0.0024636820890009403\n",
      "Episode Reward: 9.0\n",
      "Step 1435 (588251) @ Episode 52/10000, loss: 0.0027625896036624916\n",
      "Episode Reward: 32.0\n",
      "Step 767 (589018) @ Episode 53/10000, loss: 0.0033588022924959666\n",
      "Episode Reward: 10.0\n",
      "Step 963 (589981) @ Episode 54/10000, loss: 0.0016912777209654454\n",
      "Episode Reward: 21.0\n",
      "Step 18 (589999) @ Episode 55/10000, loss: 0.0043867975473403933\n",
      "Copied model parameters to target network.\n",
      "Step 833 (590814) @ Episode 55/10000, loss: 0.0037047625519335275\n",
      "Episode Reward: 16.0\n",
      "Step 1000 (591814) @ Episode 56/10000, loss: 0.0027993228286504745\n",
      "Episode Reward: 20.0\n",
      "Step 748 (592562) @ Episode 57/10000, loss: 0.0020372213330119854\n",
      "Episode Reward: 19.0\n",
      "Step 805 (593367) @ Episode 58/10000, loss: 0.0017923295963555574\n",
      "Episode Reward: 21.0\n",
      "Step 906 (594273) @ Episode 59/10000, loss: 0.0041341111063957214\n",
      "Episode Reward: 15.0\n",
      "Step 839 (595112) @ Episode 60/10000, loss: 0.0276703368872404153\n",
      "Episode Reward: 20.0\n",
      "Step 1068 (596180) @ Episode 61/10000, loss: 0.0028941293712705374\n",
      "Episode Reward: 20.0\n",
      "Step 847 (597027) @ Episode 62/10000, loss: 0.0017059341771528125\n",
      "Episode Reward: 20.0\n",
      "Step 977 (598004) @ Episode 63/10000, loss: 0.0026467416901141405\n",
      "Episode Reward: 20.0\n",
      "Step 871 (598875) @ Episode 64/10000, loss: 0.0046098898164927967\n",
      "Episode Reward: 17.0\n",
      "Step 1074 (599949) @ Episode 65/10000, loss: 0.0044169211760163312\n",
      "Episode Reward: 21.0\n",
      "Step 50 (599999) @ Episode 66/10000, loss: 0.0039318324998021137\n",
      "Copied model parameters to target network.\n",
      "Step 757 (600706) @ Episode 66/10000, loss: 0.0036901687271893024\n",
      "Episode Reward: 14.0\n",
      "Step 566 (601272) @ Episode 67/10000, loss: 0.0034878104925155645\n",
      "Episode Reward: 8.0\n",
      "Step 835 (602107) @ Episode 68/10000, loss: 0.0017091580666601658\n",
      "Episode Reward: 13.0\n",
      "Step 841 (602948) @ Episode 69/10000, loss: 0.0062424857169389725\n",
      "Episode Reward: 14.0\n",
      "Step 624 (603572) @ Episode 70/10000, loss: 0.0076144021004438456\n",
      "Episode Reward: 9.0\n",
      "Step 986 (604558) @ Episode 71/10000, loss: 0.0059645371511578567\n",
      "Episode Reward: 19.0\n",
      "Step 1061 (605619) @ Episode 72/10000, loss: 0.0031851562671363354\n",
      "Episode Reward: 28.0\n",
      "Step 406 (606025) @ Episode 73/10000, loss: 0.0025556753389537334\n",
      "Episode Reward: 5.0\n",
      "Step 588 (606613) @ Episode 74/10000, loss: 0.0024012052454054356\n",
      "Episode Reward: 10.0\n",
      "Step 1296 (607909) @ Episode 75/10000, loss: 0.0050813015550374985\n",
      "Episode Reward: 24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 05:43:22,037] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000075.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 623 (608532) @ Episode 76/10000, loss: 0.0037103537470102317\n",
      "Episode Reward: 9.0\n",
      "Step 740 (609272) @ Episode 77/10000, loss: 0.0024918932467699053\n",
      "Episode Reward: 14.0\n",
      "Step 727 (609999) @ Episode 78/10000, loss: 0.0027835727669298653\n",
      "Copied model parameters to target network.\n",
      "Step 1049 (610321) @ Episode 78/10000, loss: 0.0031819832511246204\n",
      "Episode Reward: 22.0\n",
      "Step 644 (610965) @ Episode 79/10000, loss: 0.0148785803467035357\n",
      "Episode Reward: 12.0\n",
      "Step 1134 (612099) @ Episode 80/10000, loss: 0.0039373808540403842\n",
      "Episode Reward: 27.0\n",
      "Step 1306 (613405) @ Episode 81/10000, loss: 0.0021230592392385006\n",
      "Episode Reward: 31.0\n",
      "Step 582 (613987) @ Episode 82/10000, loss: 0.0039422130212187775\n",
      "Episode Reward: 8.0\n",
      "Step 924 (614911) @ Episode 83/10000, loss: 0.0039977175183594233\n",
      "Episode Reward: 15.0\n",
      "Step 1067 (615978) @ Episode 84/10000, loss: 0.0040218681097030647\n",
      "Episode Reward: 15.0\n",
      "Step 771 (616749) @ Episode 85/10000, loss: 0.0035931358579546213\n",
      "Episode Reward: 14.0\n",
      "Step 944 (617693) @ Episode 86/10000, loss: 0.0029595335945487022\n",
      "Episode Reward: 17.0\n",
      "Step 621 (618314) @ Episode 87/10000, loss: 0.0033166741486638784\n",
      "Episode Reward: 11.0\n",
      "Step 821 (619135) @ Episode 88/10000, loss: 0.00144028617069125187\n",
      "Episode Reward: 16.0\n",
      "Step 700 (619835) @ Episode 89/10000, loss: 0.0047963792458176612\n",
      "Episode Reward: 12.0\n",
      "Step 164 (619999) @ Episode 90/10000, loss: 0.0041030081920325765\n",
      "Copied model parameters to target network.\n",
      "Step 742 (620577) @ Episode 90/10000, loss: 0.0022403611801564693\n",
      "Episode Reward: 15.0\n",
      "Step 563 (621140) @ Episode 91/10000, loss: 0.0046422462910413745\n",
      "Episode Reward: 8.0\n",
      "Step 836 (621976) @ Episode 92/10000, loss: 0.0014391134027391672\n",
      "Episode Reward: 18.0\n",
      "Step 539 (622515) @ Episode 93/10000, loss: 0.0051049808971583845\n",
      "Episode Reward: 7.0\n",
      "Step 707 (623222) @ Episode 94/10000, loss: 0.0019782918971031904\n",
      "Episode Reward: 11.0\n",
      "Step 1295 (624517) @ Episode 95/10000, loss: 0.0021552341058850291\n",
      "Episode Reward: 20.0\n",
      "Step 775 (625292) @ Episode 96/10000, loss: 0.0018084492767229676\n",
      "Episode Reward: 16.0\n",
      "Step 630 (625922) @ Episode 97/10000, loss: 0.0086044427007436757\n",
      "Episode Reward: 13.0\n",
      "Step 574 (626496) @ Episode 98/10000, loss: 0.0010007658274844289\n",
      "Episode Reward: 8.0\n",
      "Step 983 (627479) @ Episode 99/10000, loss: 0.0045891739428043365\n",
      "Episode Reward: 16.0\n",
      "Step 792 (628271) @ Episode 100/10000, loss: 0.0014792182482779026\n",
      "Episode Reward: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 07:50:43,492] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 848 (629119) @ Episode 101/10000, loss: 0.0017954665236175069\n",
      "Episode Reward: 17.0\n",
      "Step 340 (629459) @ Episode 102/10000, loss: 0.0019484555814415216\n",
      "Episode Reward: 4.0\n",
      "Step 540 (629999) @ Episode 103/10000, loss: 0.0030596330761909485\n",
      "Copied model parameters to target network.\n",
      "Step 923 (630382) @ Episode 103/10000, loss: 0.0059623103588819554\n",
      "Episode Reward: 19.0\n",
      "Step 916 (631298) @ Episode 104/10000, loss: 0.0086729293689131746\n",
      "Episode Reward: 14.0\n",
      "Step 869 (632167) @ Episode 105/10000, loss: 0.0068852454423904426\n",
      "Episode Reward: 18.0\n",
      "Step 685 (632852) @ Episode 106/10000, loss: 0.0052137495949864395\n",
      "Episode Reward: 11.0\n",
      "Step 1123 (633975) @ Episode 107/10000, loss: 0.0028420747257769108\n",
      "Episode Reward: 18.0\n",
      "Step 1304 (635279) @ Episode 108/10000, loss: 0.0019181842217221856\n",
      "Episode Reward: 45.0\n",
      "Step 783 (636062) @ Episode 109/10000, loss: 0.0026263399049639784\n",
      "Episode Reward: 12.0\n",
      "Step 784 (636846) @ Episode 110/10000, loss: 0.0032066460698843002\n",
      "Episode Reward: 17.0\n",
      "Step 944 (637790) @ Episode 111/10000, loss: 0.0305266585201025146\n",
      "Episode Reward: 15.0\n",
      "Step 1095 (638885) @ Episode 112/10000, loss: 0.0018084775656461716\n",
      "Episode Reward: 26.0\n",
      "Step 969 (639854) @ Episode 113/10000, loss: 0.0054103303700685506\n",
      "Episode Reward: 14.0\n",
      "Step 145 (639999) @ Episode 114/10000, loss: 0.0057842647656798365\n",
      "Copied model parameters to target network.\n",
      "Step 707 (640561) @ Episode 114/10000, loss: 0.0867370143532753626\n",
      "Episode Reward: 11.0\n",
      "Step 872 (641433) @ Episode 115/10000, loss: 0.0042752865701913835\n",
      "Episode Reward: 20.0\n",
      "Step 618 (642051) @ Episode 116/10000, loss: 0.0156188467517495162\n",
      "Episode Reward: 14.0\n",
      "Step 868 (642919) @ Episode 117/10000, loss: 0.0040222057141363624\n",
      "Episode Reward: 13.0\n",
      "Step 749 (643668) @ Episode 118/10000, loss: 0.0057359063066542156\n",
      "Episode Reward: 14.0\n",
      "Step 911 (644579) @ Episode 119/10000, loss: 0.0062893242575228215\n",
      "Episode Reward: 16.0\n",
      "Step 856 (645435) @ Episode 120/10000, loss: 0.0186956245452165644\n",
      "Episode Reward: 15.0\n",
      "Step 799 (646234) @ Episode 121/10000, loss: 0.0016383975744247437\n",
      "Episode Reward: 14.0\n",
      "Step 1147 (647381) @ Episode 122/10000, loss: 0.0341969393193721873\n",
      "Episode Reward: 22.0\n",
      "Step 977 (648358) @ Episode 123/10000, loss: 0.0015591233968734741\n",
      "Episode Reward: 15.0\n",
      "Step 873 (649231) @ Episode 124/10000, loss: 0.0081463418900966647\n",
      "Episode Reward: 15.0\n",
      "Step 768 (649999) @ Episode 125/10000, loss: 0.0032661964651197195\n",
      "Copied model parameters to target network.\n",
      "Step 882 (650113) @ Episode 125/10000, loss: 0.0083510000258684164\n",
      "Episode Reward: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 15:28:45,976] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1026 (651139) @ Episode 126/10000, loss: 0.0237857326865196238\n",
      "Episode Reward: 15.0\n",
      "Step 1174 (652313) @ Episode 127/10000, loss: 0.0026527259033173323\n",
      "Episode Reward: 21.0\n",
      "Step 850 (653163) @ Episode 128/10000, loss: 0.0067873364314436917\n",
      "Episode Reward: 21.0\n",
      "Step 718 (653881) @ Episode 129/10000, loss: 0.0030155994463711977\n",
      "Episode Reward: 11.0\n",
      "Step 923 (654804) @ Episode 130/10000, loss: 0.0160769317299127585\n",
      "Episode Reward: 19.0\n",
      "Step 392 (655196) @ Episode 131/10000, loss: 0.0025855023413896563\n",
      "Episode Reward: 4.0\n",
      "Step 1089 (656285) @ Episode 132/10000, loss: 0.0189290046691894532\n",
      "Episode Reward: 19.0\n",
      "Step 955 (657240) @ Episode 133/10000, loss: 0.0048877447843551645\n",
      "Episode Reward: 18.0\n",
      "Step 834 (658074) @ Episode 134/10000, loss: 0.0048808818683028224\n",
      "Episode Reward: 16.0\n",
      "Step 826 (658900) @ Episode 135/10000, loss: 0.0046622594818472862\n",
      "Episode Reward: 17.0\n",
      "Step 863 (659763) @ Episode 136/10000, loss: 0.0039520068094134335\n",
      "Episode Reward: 18.0\n",
      "Step 236 (659999) @ Episode 137/10000, loss: 0.0024052320513874292\n",
      "Copied model parameters to target network.\n",
      "Step 1004 (660767) @ Episode 137/10000, loss: 0.0215857885777950324\n",
      "Episode Reward: 24.0\n",
      "Step 688 (661455) @ Episode 138/10000, loss: 0.0030659497715532786\n",
      "Episode Reward: 11.0\n",
      "Step 809 (662264) @ Episode 139/10000, loss: 0.0075722448527812963\n",
      "Episode Reward: 14.0\n",
      "Step 580 (662844) @ Episode 140/10000, loss: 0.0086995493620634083\n",
      "Episode Reward: 8.0\n",
      "Step 911 (663755) @ Episode 141/10000, loss: 0.0038202793803066015\n",
      "Episode Reward: 16.0\n",
      "Step 926 (664681) @ Episode 142/10000, loss: 0.0074340063147246846\n",
      "Episode Reward: 14.0\n",
      "Step 684 (665365) @ Episode 143/10000, loss: 0.0034056489821523433\n",
      "Episode Reward: 7.0\n",
      "Step 718 (666083) @ Episode 144/10000, loss: 0.0068218391388654714\n",
      "Episode Reward: 11.0\n",
      "Step 1015 (667098) @ Episode 145/10000, loss: 0.0038467389531433582\n",
      "Episode Reward: 21.0\n",
      "Step 731 (667829) @ Episode 146/10000, loss: 0.0078858472406864177\n",
      "Episode Reward: 15.0\n",
      "Step 741 (668570) @ Episode 147/10000, loss: 0.0017490875907242298\n",
      "Episode Reward: 13.0\n",
      "Step 686 (669256) @ Episode 148/10000, loss: 0.0035644676536321643\n",
      "Episode Reward: 11.0\n",
      "Step 743 (669999) @ Episode 149/10000, loss: 0.0059573366306722167\n",
      "Copied model parameters to target network.\n",
      "Step 859 (670115) @ Episode 149/10000, loss: 0.0035754181444644934\n",
      "Episode Reward: 18.0\n",
      "Step 902 (671017) @ Episode 150/10000, loss: 0.0096403723582625395\n",
      "Episode Reward: 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-22 18:00:39,509] Starting new video recorder writing to C:\\Users\\Brad Pospeck\\Desktop\\Classes\\Senior II\\CS480\\Project\\experiments\\Breakout-v0\\monitor\\openaigym.video.0.13868.video000150.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 818 (671835) @ Episode 151/10000, loss: 0.0070923282764852058\n",
      "Episode Reward: 13.0\n",
      "Step 27 (671862) @ Episode 152/10000, loss: 0.0026051653549075127"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c03f7703bc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                     record_video_every=25):\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1417199c67b7>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[1;31m# Perform gradient descent update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mstates_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-106cb451b4f9>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m     97\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[1;32m     98\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Pospeck\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=5000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32,\n",
    "                                    record_video_every=25):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
