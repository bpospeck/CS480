{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bradley Pospeck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be looking into deep reinforcement learning for my project. Using an environment from [OpenAI Gym](https://gym.openai.com/), I will compare different RL implementations of Deep Q Learning (DQN). \n",
    "\n",
    "The environment I plan on using is `Breakout-v0`. This environment is from the old Atari video game system, and is also known as brick-breaker today. The algorithms will learn from scratch how to play the game. The environment inputs will be images at different time steps. There will be 4 possible actions to take in the environment:\n",
    "    1. Move left\n",
    "    2. Move right\n",
    "    3. Fire\n",
    "    4. Do nothing\n",
    "\n",
    "I intend to compare the in class implementation with 1 or more implementations from online. Multiple algorithms will make it easier to analyze and figure out what adjustments and methods work best when programs determine how to play Breakout successfully. To start, I'll only be comparing with 1 other implementation. Others will be used for additional analysis only if time permits. It would make the most sense if some similar parameters, such as epsilon values, are made to be the same between algorithms. That way, comparisons of their results would be more meaningful. It will also be worth noting differences between which libraries are used, and how calculations are made, as those can affect algorithm efficiency. \n",
    "\n",
    "If time allows, it could also be interesting to try different RL algorithms than DQN. I'm not so sure there will be time for that, but it is something I plan on keeping in mind just in case.\n",
    "\n",
    "Prior to this class, I have been interested in the ideas of reinforcement learning. Since the introduction to it in this class, I still feel much the same way. The idea that a machine or a program can teach itself how to play games, often at super human levels, is incredible. Sure, it may take a lot of time and resources to accomplish its goals, but it does so on its own and without having any prior knowledge of how to achieve its task. I find that amazing. For the project, I wanted to apply reinforcement learning to some kind of game. I wanted the game to be a little more involved than just a text based maze or tic-tac-toe, but also within reason. My computer has relatively limited computing power, and this atari version of Breakout on OpenAI Gym seems like it should be achievable without running tests for multiple days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, I will be using `Breakout-v0` from OpenAI Gym as the environment that will be tested on. The algorithm I plan on using from online can be found from github user \"dennybritz\" at [this location.](https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning%20Solution.ipynb) The in class implementations will also be used.\n",
    "\n",
    "My plan is to start by familiarizing myself with OpenAI Gym and the `Breakout-v0` environment before working with the algorithms. After I feel comfortable with that, I will be running the online algorithm first to see how it performs. After finishing up with that, I will adapt the in-class implementation to run with OpenAI Gym and run it. From there I plan to analyze the algorithms and compare the performances between them. \n",
    "\n",
    "If time allows, it would be interesting to vary the different parameters such as epsilon, epsilon decay, batch sizes, etc.. This would allow further in depth analysis of how the algorithms perform with the given task. Having an opportunity at the end to throw in a different RL algorithm would be ideal as well. Seeing different algorithms altogether run may better reveal certain strengths and weaknesses of each algorithm type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "\n",
    "I suspect the differences in libraries used will indeed have an effect on the run times of both DQN implementations. The in class one relies mainly on numpy, while the online code relies heavily on the [TensorFlow library](https://www.tensorflow.org/). At this point, I still know relatively little about RL algorithms and it is difficult to speculate. However, since every problem in machine learning is unique, I would assume that there may be certain ways to tweak algorithms in order to better tailor them to a specific problem. Since the online algorithm was made for the `Breakout-v0` environment, I would not be surprised if it ends up performing better than the in class algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "  * 4/10-4/16: Play around with OpenAI Gym to familiarize myself with how it works and how to use it. Plan to try running the online algorithm found and see how it performs.\n",
    "  * 4/17-4/23: Finish running online algorithm and look into setting up the in class algorithm with OpenAI Gym.\n",
    "  * 4/24-4/30: Finish setting up in class algorithm and run it. Compare and contrast the 2 algorithms.\n",
    "  * 5/1-due date: Hopefully the base goal will be achieved by now. If so, here I can play around with the parameters and algorithms a bit more to try and improve them. Other possibilities would be to try using other implementations or other algorithms entirely for additional comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
